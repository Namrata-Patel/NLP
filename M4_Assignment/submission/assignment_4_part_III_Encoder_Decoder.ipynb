{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 4 - Encoder Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-24 13:01:02.100861: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-24 13:01:02.897777: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-24 13:01:07.165077: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "[nltk_data] Downloading package names to /home/namrata/nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import nltk\n",
    "nltk.download('names')\n",
    "from nltk.corpus import names\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer, MultiHeadAttention, Dense, Embedding, LayerNormalization, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load names\n",
    "male_names = names.words('male.txt')\n",
    "female_names = names.words('female.txt')\n",
    "\n",
    "# Label data\n",
    "data = [(name.lower(), 1) for name in male_names] + [(name.lower(), 0) for name in female_names]\n",
    "\n",
    "# Shuffle data\n",
    "import random\n",
    "random.shuffle(data)\n",
    "\n",
    "# Split into names and labels\n",
    "names, labels = zip(*data)\n",
    "labels = tf.keras.utils.to_categorical(labels, num_classes=2)\n",
    "\n",
    "# Tokenization and Padding\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(names)\n",
    "seqs = tokenizer.texts_to_sequences(names)\n",
    "max_len = max(len(seq) for seq in seqs)\n",
    "names_padded = tf.keras.preprocessing.sequence.pad_sequences(seqs, maxlen=max_len, padding='post')\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(Layer):\n",
    "    def __init__(self, embed_dim, dff, num_heads, dropout_rate=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # Multi-head attention with 'num_heads' and dimension per head 'embed_dim'\n",
    "        self.attention = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        # Feedforward network consists of two dense layers with a dropout in between\n",
    "        self.dense_proj = tf.keras.Sequential([\n",
    "            Dense(dff, activation=\"relu\"),  # First dense layer with more units (dff)\n",
    "            Dropout(dropout_rate),          # Dropout layer for regularization\n",
    "            Dense(embed_dim)                # Second dense layer to bring dimension back to 'embed_dim'\n",
    "        ])\n",
    "        # Layer normalization to help stabilize the learning process\n",
    "        self.layernorm1 = LayerNormalization()\n",
    "        self.layernorm2 = LayerNormalization()\n",
    "        # Dropout layer added for additional regularization\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, training=False, mask=None):\n",
    "        # Attention mechanism\n",
    "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
    "        # Apply dropout to attention output\n",
    "        attention_output = self.dropout(attention_output, training=training)\n",
    "        # Add & norm layer after adding residual connection\n",
    "        proj_input = self.layernorm1(inputs + attention_output)\n",
    "        # Pass through the feedforward network\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        # Apply dropout to the output of the feedforward network\n",
    "        proj_output = self.dropout(proj_output, training=training)\n",
    "        # Second add & norm layer after adding residual connection\n",
    "        return self.layernorm2(proj_input + proj_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(Layer):\n",
    "    def __init__(self, embed_dim, dff, num_heads, dropout_rate=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # First multi-head attention layer (self attention)\n",
    "        self.attention1 = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        # Second multi-head attention layer (encoder-decoder attention)\n",
    "        self.attention2 = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        # Feedforward network applied at the decoder stage\n",
    "        self.dense_proj = tf.keras.Sequential([\n",
    "            Dense(dff, activation=\"relu\"),  # Dense layer with 'dff' units\n",
    "            Dropout(dropout_rate),          # Dropout for regularization\n",
    "            Dense(embed_dim)                # Dense layer to project back to embedding dimension\n",
    "        ])\n",
    "        # Three layer normalization steps to stabilize the learning\n",
    "        self.layernorm1 = LayerNormalization()\n",
    "        self.layernorm2 = LayerNormalization()\n",
    "        self.layernorm3 = LayerNormalization()\n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, training=False, mask=None):\n",
    "        # Self-attention on decoder inputs\n",
    "        attention_output1 = self.attention1(inputs, inputs, attention_mask=mask)\n",
    "        # Apply dropout after self-attention\n",
    "        attention_output1 = self.dropout(attention_output1, training=training)\n",
    "        # First add & norm step\n",
    "        attention_input1 = self.layernorm1(inputs + attention_output1)\n",
    "        # Encoder-decoder attention, using output from the encoder as key and value\n",
    "        attention_output2 = self.attention2(attention_input1, encoder_outputs, attention_mask=mask)\n",
    "        # Apply dropout after encoder-decoder attention\n",
    "        attention_output2 = self.dropout(attention_output2, training=training)\n",
    "        # Second add & norm step\n",
    "        attention_input2 = self.layernorm2(attention_input1 + attention_output2)\n",
    "        # Feedforward network\n",
    "        proj_output = self.dense_proj(attention_input2)\n",
    "        # Apply dropout to the output of feedforward network\n",
    "        proj_output = self.dropout(proj_output, training=training)\n",
    "        # Final add & norm step\n",
    "        return self.layernorm3(attention_input2 + proj_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel:\n",
    "    def __init__(self, vocab_size, max_len, embed_dim=32, dff=64, num_heads=2, num_classes=2, dropout_rate=0.1):\n",
    "        # Constructor for the Model. Set up the Vocab size, drop out rate etc. \n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_len = max_len\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dff = dff\n",
    "        self.num_heads = num_heads\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        inputs = tf.keras.Input(shape=(self.max_len,))\n",
    "        x = Embedding(input_dim=self.vocab_size, output_dim=self.embed_dim)(inputs)\n",
    "        x = TransformerEncoder(self.embed_dim, self.dff, self.num_heads, self.dropout_rate)(x) # setup Encoder\n",
    "        x = TransformerDecoder(self.embed_dim, self.dff, self.num_heads, self.dropout_rate)(x, x) # Setup Decoder\n",
    "        x = Dense(self.num_classes, activation=\"softmax\")(x[:, 0, :])\n",
    "        model = Model(inputs, x)\n",
    "        return model\n",
    "\n",
    "    def compile(self, optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]):\n",
    "        self.model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "    def fit(self, *args, **kwargs):\n",
    "        return self.model.fit(*args, **kwargs)\n",
    "\n",
    "    def predict(self, *args, **kwargs):\n",
    "        return self.model.predict(*args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-24 13:02:53.470493: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-24 13:02:53.522220: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the TransformerModel\n",
    "transformer = TransformerModel(vocab_size=vocab_size, max_len=max_len)\n",
    "\n",
    "# Compile the model\n",
    "transformer.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 46ms/step - accuracy: 0.7793 - loss: 0.4744\n",
      "Epoch 2/10\n",
      "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - accuracy: 0.7787 - loss: 0.4700\n",
      "Epoch 3/10\n",
      "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - accuracy: 0.7802 - loss: 0.4639\n",
      "Epoch 4/10\n",
      "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 46ms/step - accuracy: 0.7773 - loss: 0.4658\n",
      "Epoch 5/10\n",
      "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 48ms/step - accuracy: 0.7837 - loss: 0.4629\n",
      "Epoch 6/10\n",
      "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 48ms/step - accuracy: 0.7860 - loss: 0.4579\n",
      "Epoch 7/10\n",
      "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - accuracy: 0.7765 - loss: 0.4647\n",
      "Epoch 8/10\n",
      "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - accuracy: 0.7829 - loss: 0.4611\n",
      "Epoch 9/10\n",
      "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 48ms/step - accuracy: 0.7841 - loss: 0.4551\n",
      "Epoch 10/10\n",
      "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 49ms/step - accuracy: 0.8032 - loss: 0.4291\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss : 0.4453 , Test Accuracy: 79.42%\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, names, labels, epochs=1, batch_size=32):\n",
    "    # Ensure the dataset is correctly prepared\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((names_padded, labels))\n",
    "    dataset = dataset.shuffle(buffer_size=len(names_padded)).batch(batch_size=32)\n",
    "\n",
    "    # Use the fit method of the model for training which handles batches and losses internally\n",
    "    history = model.fit(dataset, epochs=10)\n",
    "\n",
    "    loss = history.history['loss'][-1]  # Get the final loss from the history\n",
    "    accuracy = history.history['accuracy'][-1] \n",
    "\n",
    "    print(\"-\"*100)\n",
    "    print(f\"Loss : {loss:.4f} , Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Execute training\n",
    "train_model(transformer, names_padded, labels, epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_names = [\"Charlie\", \"Martina\", \"John\", \"Elizabeth\",\"Rita\",\"Harry\"]\n",
    "\n",
    "# Tokenize and pad the new names\n",
    "new_seqs = tokenizer.texts_to_sequences(new_names)\n",
    "new_names_padded = tf.keras.preprocessing.sequence.pad_sequences(new_seqs, maxlen=max_len, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 854ms/step\n",
      "[[0.8492838  0.15071622]\n",
      " [0.7928341  0.20716597]\n",
      " [0.16434601 0.83565396]\n",
      " [0.7242352  0.27576482]\n",
      " [0.91736984 0.08263018]\n",
      " [0.34865716 0.65134287]]\n"
     ]
    }
   ],
   "source": [
    "# Predict using the trained model\n",
    "predictions = transformer.predict(new_names_padded)\n",
    "\n",
    "# Output the predictions\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Charlie, Predicted Gender: Female\n",
      "Name: Martina, Predicted Gender: Female\n",
      "Name: John, Predicted Gender: Male\n",
      "Name: Elizabeth, Predicted Gender: Female\n",
      "Name: Rita, Predicted Gender: Female\n",
      "Name: Harry, Predicted Gender: Male\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = tf.argmax(predictions, axis=1).numpy()\n",
    "class_names = [\"Female\", \"Male\"]\n",
    "\n",
    "# Print predicted class names:\n",
    "for name, label in zip(new_names, predicted_labels):\n",
    "    print(f\"Name: {name}, Predicted Gender: {class_names[label]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### References:\n",
    "\n",
    "\n",
    "https://kikaben.com/transformers-encoder-decoder/#google_vignette\n",
    "\n",
    "\n",
    "https://machinelearningmastery.com/implementing-the-transformer-encoder-from-scratch-in-tensorflow-and-keras/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
